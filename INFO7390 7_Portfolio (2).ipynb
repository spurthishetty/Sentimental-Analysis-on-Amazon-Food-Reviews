{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sentiment Analysis on Amazon Food Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Food Reviews Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed graph along a sequence. This allows it to exhibit dynamic temporal behavior for a time sequence. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/user/Desktop'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm and Deep Learning\n",
    "In order to predict if a review is positive or negative we will use a recurrent neural network. Most of the methods for sentiment analysis look at each word individually, attribute positive points for positive words and negative points for negative words, and then total the points. This is the lexicon approach. However the problem with this method is that it ignores the sequence of the words, which can lead to the loss of important information. The RNN approach can understand subtleties because it doesnâ€™t analyze text at face value. It creates abstract representations of what it learned.\n",
    "\n",
    "We will use Long Short Term Memory Networks (LSTM), which is a special case of RNN. The main advantage of LSTM is that it can memorize information. When there's too large of a gap between two pieces of information, RNN is unable to learn to connect information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, LSTM\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "%matplotlib inline\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 14, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mns = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"FoodReview.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492346</td>\n",
       "      <td>B001VNP0Y6</td>\n",
       "      <td>AEZRVY3CV52UZ</td>\n",
       "      <td>Corey Wright \"Democratus\"</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1320537600</td>\n",
       "      <td>Just what I needed</td>\n",
       "      <td>I got them in a very timely manner and they're...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>343811</td>\n",
       "      <td>B008J1HO4C</td>\n",
       "      <td>A18VZYSLOUO060</td>\n",
       "      <td>D. Simpson \"Frugal SOB\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1204329600</td>\n",
       "      <td>Most excellent oatmeal</td>\n",
       "      <td>McCann's Steel Cut Oatmeal is the perfect brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>347257</td>\n",
       "      <td>B000FFIL60</td>\n",
       "      <td>A28Y1M7GRG0I9M</td>\n",
       "      <td>Real Comments \"Lin\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1318464000</td>\n",
       "      <td>Quality Tea</td>\n",
       "      <td>I would recommend it. Quality pearls and a few...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>225529</td>\n",
       "      <td>B002ZX1U9A</td>\n",
       "      <td>A245HC4T5J97WG</td>\n",
       "      <td>Logan DeAngelis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1306540800</td>\n",
       "      <td>Great Coffee at a Great price</td>\n",
       "      <td>I have to admit, I first purchased these Hazel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>485488</td>\n",
       "      <td>B001RVFERK</td>\n",
       "      <td>AVABPJCKE2MR5</td>\n",
       "      <td>spal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1314921600</td>\n",
       "      <td>Popchips Jalapeno flavor</td>\n",
       "      <td>I bought a case of jalapeno chips as I love th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId                ProfileName  \\\n",
       "0  492346  B001VNP0Y6   AEZRVY3CV52UZ  Corey Wright \"Democratus\"   \n",
       "1  343811  B008J1HO4C  A18VZYSLOUO060    D. Simpson \"Frugal SOB\"   \n",
       "2  347257  B000FFIL60  A28Y1M7GRG0I9M        Real Comments \"Lin\"   \n",
       "3  225529  B002ZX1U9A  A245HC4T5J97WG            Logan DeAngelis   \n",
       "4  485488  B001RVFERK   AVABPJCKE2MR5                       spal   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Prediction        Time  \\\n",
       "0                     3                       3           5  1320537600   \n",
       "1                     2                       2           5  1204329600   \n",
       "2                     0                       0           4  1318464000   \n",
       "3                     0                       0           5  1306540800   \n",
       "4                     0                       0           4  1314921600   \n",
       "\n",
       "                         Summary  \\\n",
       "0             Just what I needed   \n",
       "1         Most excellent oatmeal   \n",
       "2                    Quality Tea   \n",
       "3  Great Coffee at a Great price   \n",
       "4       Popchips Jalapeno flavor   \n",
       "\n",
       "                                                Text  \n",
       "0  I got them in a very timely manner and they're...  \n",
       "1  McCann's Steel Cut Oatmeal is the perfect brea...  \n",
       "2  I would recommend it. Quality pearls and a few...  \n",
       "3  I have to admit, I first purchased these Hazel...  \n",
       "4  I bought a case of jalapeno chips as I love th...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Prediction', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split table into training table\n",
    "training_table = data[['Id','HelpfulnessNumerator','HelpfulnessDenominator', 'Prediction', 'Time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output: positive or negative?\n",
    "Finally, we will predict if the review is positive or negative using the sigmoid function. The advantage of the function is that it's bound between 0 and 1 and can be interpreted as a probability of success. For example, we can estimate the probability that a review is positive. At each step, we have an output. However, we only care about the final output that predicts the sentiment at the end of the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492346</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1320537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>343811</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1204329600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>347257</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1318464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>225529</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1306540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>485488</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1314921600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id  HelpfulnessNumerator  HelpfulnessDenominator  Prediction  \\\n",
       "0  492346                     3                       3           5   \n",
       "1  343811                     2                       2           5   \n",
       "2  347257                     0                       0           4   \n",
       "3  225529                     0                       0           5   \n",
       "4  485488                     0                       0           4   \n",
       "\n",
       "         Time  \n",
       "0  1320537600  \n",
       "1  1204329600  \n",
       "2  1318464000  \n",
       "3  1306540800  \n",
       "4  1314921600  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "training_table = training_table.iloc[:, 2:3].values\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mns = MinMaxScaler()\n",
    "training_table = mns.fit_transform(training_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters\n",
    "For our recurrent neural network we will have to tune the following hyperparameters:\n",
    "\n",
    "Optimizer hyperparameters:\n",
    "\n",
    "1.) Learning rate\n",
    "2.) Minibatch size\n",
    "3.) Number of epochs\n",
    "\n",
    "Model hyperparameters:\n",
    "\n",
    "1.) Number of hidden layers (LSTM layers)\n",
    "\n",
    "2.) Number of units in the LSTM cells\n",
    "\n",
    "3.) Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the inputs and the ouputs\n",
    "x_train =  training_table[0:999]\n",
    "y_train = training_table[1:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 1, 1)\n",
      "(999, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping\n",
    "x_train = x_train.reshape(999,1,1)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory\n",
    "The next step is to pass the embedded words to the LSTM cells. In the graph, you can observe that there are connections between the first, second and third LSTM cells. In contrast to other models that assume that all the inputs are independent of each other, these connections here allow us to pass information contained in the sequence of words. One of the strengths of the LSTM cell is the ability to add or remove information to the cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "regressor.add(LSTM(12, activation='relu', input_shape=(None,1)))\n",
    "regressor.add(Dense(8, activation='softmax'))\n",
    "regressor.add(Dense(1))\n",
    "regressor.compile(optimizer='adadelta', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "999/999 [==============================] - 1s 885us/step - loss: 5.0509e-04\n",
      "Epoch 2/20\n",
      "999/999 [==============================] - 0s 489us/step - loss: 2.7751e-05\n",
      "Epoch 3/20\n",
      "999/999 [==============================] - 0s 392us/step - loss: 2.7957e-05\n",
      "Epoch 4/20\n",
      "999/999 [==============================] - 0s 428us/step - loss: 2.8000e-05\n",
      "Epoch 5/20\n",
      "999/999 [==============================] - 0s 411us/step - loss: 2.8153e-05\n",
      "Epoch 6/20\n",
      "999/999 [==============================] - 0s 414us/step - loss: 2.8068e-05\n",
      "Epoch 7/20\n",
      "999/999 [==============================] - 0s 411us/step - loss: 2.8173e-05\n",
      "Epoch 8/20\n",
      "999/999 [==============================] - 0s 420us/step - loss: 2.8092e-05\n",
      "Epoch 9/20\n",
      "999/999 [==============================] - 0s 422us/step - loss: 2.8661e-05\n",
      "Epoch 10/20\n",
      "999/999 [==============================] - 0s 421us/step - loss: 2.8412e-05\n",
      "Epoch 11/20\n",
      "999/999 [==============================] - 0s 421us/step - loss: 2.8253e-05\n",
      "Epoch 12/20\n",
      "999/999 [==============================] - 0s 391us/step - loss: 2.8304e-05\n",
      "Epoch 13/20\n",
      "999/999 [==============================] - 0s 402us/step - loss: 2.8243e-05\n",
      "Epoch 14/20\n",
      "999/999 [==============================] - 0s 400us/step - loss: 2.8416e-05\n",
      "Epoch 15/20\n",
      "999/999 [==============================] - 1s 510us/step - loss: 2.8469e-05\n",
      "Epoch 16/20\n",
      "999/999 [==============================] - 0s 490us/step - loss: 2.8243e-05\n",
      "Epoch 17/20\n",
      "999/999 [==============================] - 0s 407us/step - loss: 2.8429e-05\n",
      "Epoch 18/20\n",
      "999/999 [==============================] - 0s 417us/step - loss: 2.7898e-05\n",
      "Epoch 19/20\n",
      "999/999 [==============================] - 0s 409us/step - loss: 2.7948e-05\n",
      "Epoch 20/20\n",
      "999/999 [==============================] - 0s 430us/step - loss: 2.8255e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1537c3a90>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(x_train, y_train, batch_size=20, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To train the RRN on the review dataset we have to create an array with 200 columns, and enough rows to fit one review per row. Then store each review consisting of integers in the array. If the number of words in the review is less than 200 words, we can pad the list with extra zeros. If the number of words in the review are more than 200 words then we can limit the review to the first 200 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding the input layer and the LSTM layer\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(128,input_shape = (None,1)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid',kernel_initializer='uniform'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = training_table[1:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "999/999 [==============================] - 1s 890us/step - loss: 0.3744\n",
      "Epoch 2/100\n",
      "999/999 [==============================] - 1s 541us/step - loss: 0.0416\n",
      "Epoch 3/100\n",
      "999/999 [==============================] - 1s 543us/step - loss: 0.0208\n",
      "Epoch 4/100\n",
      "999/999 [==============================] - 1s 545us/step - loss: 0.0170\n",
      "Epoch 5/100\n",
      "999/999 [==============================] - 1s 534us/step - loss: 0.0163\n",
      "Epoch 6/100\n",
      "999/999 [==============================] - 1s 540us/step - loss: 0.0159\n",
      "Epoch 7/100\n",
      "999/999 [==============================] - 1s 526us/step - loss: 0.0158\n",
      "Epoch 8/100\n",
      "999/999 [==============================] - 1s 515us/step - loss: 0.0155\n",
      "Epoch 9/100\n",
      "999/999 [==============================] - 1s 560us/step - loss: 0.0157\n",
      "Epoch 10/100\n",
      "999/999 [==============================] - 1s 544us/step - loss: 0.0158\n",
      "Epoch 11/100\n",
      "999/999 [==============================] - 1s 565us/step - loss: 0.0156\n",
      "Epoch 12/100\n",
      "999/999 [==============================] - 1s 701us/step - loss: 0.0155\n",
      "Epoch 13/100\n",
      "999/999 [==============================] - 1s 612us/step - loss: 0.0157\n",
      "Epoch 14/100\n",
      "999/999 [==============================] - 1s 558us/step - loss: 0.0158\n",
      "Epoch 15/100\n",
      "999/999 [==============================] - 1s 577us/step - loss: 0.0158\n",
      "Epoch 16/100\n",
      "999/999 [==============================] - 1s 660us/step - loss: 0.0156\n",
      "Epoch 17/100\n",
      "999/999 [==============================] - 1s 560us/step - loss: 0.0155\n",
      "Epoch 18/100\n",
      "999/999 [==============================] - 1s 611us/step - loss: 0.0156\n",
      "Epoch 19/100\n",
      "999/999 [==============================] - 1s 555us/step - loss: 0.0156\n",
      "Epoch 20/100\n",
      "999/999 [==============================] - 1s 562us/step - loss: 0.0155\n",
      "Epoch 21/100\n",
      "999/999 [==============================] - 1s 553us/step - loss: 0.0158\n",
      "Epoch 22/100\n",
      "999/999 [==============================] - 1s 559us/step - loss: 0.0155\n",
      "Epoch 23/100\n",
      "999/999 [==============================] - 1s 558us/step - loss: 0.0157\n",
      "Epoch 24/100\n",
      "999/999 [==============================] - 1s 558us/step - loss: 0.0156\n",
      "Epoch 25/100\n",
      "999/999 [==============================] - 1s 543us/step - loss: 0.0158\n",
      "Epoch 26/100\n",
      "999/999 [==============================] - 1s 559us/step - loss: 0.0156\n",
      "Epoch 27/100\n",
      "999/999 [==============================] - 1s 567us/step - loss: 0.0157\n",
      "Epoch 28/100\n",
      "999/999 [==============================] - 1s 580us/step - loss: 0.0159\n",
      "Epoch 29/100\n",
      "999/999 [==============================] - 1s 546us/step - loss: 0.0157\n",
      "Epoch 30/100\n",
      "999/999 [==============================] - 1s 560us/step - loss: 0.0155\n",
      "Epoch 31/100\n",
      "999/999 [==============================] - 1s 565us/step - loss: 0.0156\n",
      "Epoch 32/100\n",
      "999/999 [==============================] - 1s 573us/step - loss: 0.0157\n",
      "Epoch 33/100\n",
      "999/999 [==============================] - 1s 561us/step - loss: 0.0155\n",
      "Epoch 34/100\n",
      "999/999 [==============================] - 1s 559us/step - loss: 0.0157\n",
      "Epoch 35/100\n",
      "999/999 [==============================] - 1s 530us/step - loss: 0.0158\n",
      "Epoch 36/100\n",
      "999/999 [==============================] - 1s 547us/step - loss: 0.0157\n",
      "Epoch 37/100\n",
      "999/999 [==============================] - 1s 555us/step - loss: 0.0155\n",
      "Epoch 38/100\n",
      "999/999 [==============================] - 1s 561us/step - loss: 0.0157\n",
      "Epoch 39/100\n",
      "999/999 [==============================] - 1s 568us/step - loss: 0.0158\n",
      "Epoch 40/100\n",
      "999/999 [==============================] - 1s 560us/step - loss: 0.0157\n",
      "Epoch 41/100\n",
      "999/999 [==============================] - 1s 567us/step - loss: 0.0156\n",
      "Epoch 42/100\n",
      "999/999 [==============================] - 1s 560us/step - loss: 0.0159\n",
      "Epoch 43/100\n",
      "999/999 [==============================] - 1s 568us/step - loss: 0.0156\n",
      "Epoch 44/100\n",
      "999/999 [==============================] - 1s 563us/step - loss: 0.0157\n",
      "Epoch 45/100\n",
      "999/999 [==============================] - 1s 568us/step - loss: 0.0156\n",
      "Epoch 46/100\n",
      "999/999 [==============================] - 1s 555us/step - loss: 0.0156\n",
      "Epoch 47/100\n",
      "999/999 [==============================] - 1s 533us/step - loss: 0.0157\n",
      "Epoch 48/100\n",
      "999/999 [==============================] - 1s 553us/step - loss: 0.0158\n",
      "Epoch 49/100\n",
      "999/999 [==============================] - 1s 572us/step - loss: 0.0156\n",
      "Epoch 50/100\n",
      "999/999 [==============================] - 1s 563us/step - loss: 0.0158\n",
      "Epoch 51/100\n",
      "999/999 [==============================] - 1s 555us/step - loss: 0.0157\n",
      "Epoch 52/100\n",
      "999/999 [==============================] - 1s 561us/step - loss: 0.0158\n",
      "Epoch 53/100\n",
      "999/999 [==============================] - 1s 568us/step - loss: 0.0159\n",
      "Epoch 54/100\n",
      "999/999 [==============================] - 1s 563us/step - loss: 0.0156\n",
      "Epoch 55/100\n",
      "999/999 [==============================] - 1s 559us/step - loss: 0.0156\n",
      "Epoch 56/100\n",
      "999/999 [==============================] - 1s 553us/step - loss: 0.0157\n",
      "Epoch 57/100\n",
      "999/999 [==============================] - 1s 557us/step - loss: 0.0156\n",
      "Epoch 58/100\n",
      "999/999 [==============================] - 1s 556us/step - loss: 0.0158\n",
      "Epoch 59/100\n",
      "999/999 [==============================] - 1s 555us/step - loss: 0.0157\n",
      "Epoch 60/100\n",
      "999/999 [==============================] - 1s 583us/step - loss: 0.0156\n",
      "Epoch 61/100\n",
      "999/999 [==============================] - 1s 536us/step - loss: 0.0157\n",
      "Epoch 62/100\n",
      "999/999 [==============================] - 1s 546us/step - loss: 0.0156\n",
      "Epoch 63/100\n",
      "999/999 [==============================] - 1s 550us/step - loss: 0.0158\n",
      "Epoch 64/100\n",
      "999/999 [==============================] - 1s 554us/step - loss: 0.0155\n",
      "Epoch 65/100\n",
      "999/999 [==============================] - 1s 542us/step - loss: 0.0156\n",
      "Epoch 66/100\n",
      "999/999 [==============================] - 1s 558us/step - loss: 0.0155\n",
      "Epoch 67/100\n",
      "999/999 [==============================] - 1s 553us/step - loss: 0.0156\n",
      "Epoch 68/100\n",
      "999/999 [==============================] - 1s 551us/step - loss: 0.0158\n",
      "Epoch 69/100\n",
      "999/999 [==============================] - 1s 552us/step - loss: 0.0156\n",
      "Epoch 70/100\n",
      "999/999 [==============================] - 1s 555us/step - loss: 0.0156\n",
      "Epoch 71/100\n",
      "999/999 [==============================] - 1s 592us/step - loss: 0.0157\n",
      "Epoch 72/100\n",
      "999/999 [==============================] - 1s 540us/step - loss: 0.0158\n",
      "Epoch 73/100\n",
      "999/999 [==============================] - 1s 558us/step - loss: 0.0155\n",
      "Epoch 74/100\n",
      "999/999 [==============================] - 1s 553us/step - loss: 0.0157\n",
      "Epoch 75/100\n",
      "999/999 [==============================] - 1s 551us/step - loss: 0.0156\n",
      "Epoch 76/100\n",
      "999/999 [==============================] - 1s 554us/step - loss: 0.0157\n",
      "Epoch 77/100\n",
      "999/999 [==============================] - 1s 561us/step - loss: 0.0157\n",
      "Epoch 78/100\n",
      "999/999 [==============================] - 1s 559us/step - loss: 0.0157\n",
      "Epoch 79/100\n",
      "999/999 [==============================] - 1s 548us/step - loss: 0.0156\n",
      "Epoch 80/100\n",
      "999/999 [==============================] - 1s 552us/step - loss: 0.0155\n",
      "Epoch 81/100\n",
      "999/999 [==============================] - 1s 554us/step - loss: 0.0159\n",
      "Epoch 82/100\n",
      "999/999 [==============================] - 1s 578us/step - loss: 0.0157\n",
      "Epoch 83/100\n",
      "999/999 [==============================] - 1s 557us/step - loss: 0.0157\n",
      "Epoch 84/100\n",
      "999/999 [==============================] - 1s 555us/step - loss: 0.0157\n",
      "Epoch 85/100\n",
      "999/999 [==============================] - 1s 553us/step - loss: 0.0157\n",
      "Epoch 86/100\n",
      "999/999 [==============================] - 1s 557us/step - loss: 0.0158\n",
      "Epoch 87/100\n",
      "999/999 [==============================] - 1s 557us/step - loss: 0.0155\n",
      "Epoch 88/100\n",
      "999/999 [==============================] - 1s 561us/step - loss: 0.0158\n",
      "Epoch 89/100\n",
      "999/999 [==============================] - 1s 590us/step - loss: 0.0158\n",
      "Epoch 90/100\n",
      "999/999 [==============================] - 1s 551us/step - loss: 0.0156\n",
      "Epoch 91/100\n",
      "999/999 [==============================] - 1s 552us/step - loss: 0.0156\n",
      "Epoch 92/100\n",
      "999/999 [==============================] - 1s 552us/step - loss: 0.0157\n",
      "Epoch 93/100\n",
      "999/999 [==============================] - 1s 561us/step - loss: 0.0157\n",
      "Epoch 94/100\n",
      "999/999 [==============================] - 1s 546us/step - loss: 0.0155\n",
      "Epoch 95/100\n",
      "999/999 [==============================] - 1s 546us/step - loss: 0.0158\n",
      "Epoch 96/100\n",
      "999/999 [==============================] - 1s 545us/step - loss: 0.0155\n",
      "Epoch 97/100\n",
      "999/999 [==============================] - 1s 555us/step - loss: 0.0156\n",
      "Epoch 98/100\n",
      "999/999 [==============================] - 1s 536us/step - loss: 0.0158\n",
      "Epoch 99/100\n",
      "999/999 [==============================] - 1s 533us/step - loss: 0.0157\n",
      "Epoch 100/100\n",
      "999/999 [==============================] - 1s 534us/step - loss: 0.0158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13d6807b8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=8, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999/999 [==============================] - 0s 211us/step\n",
      "\n",
      "['loss']: 1.53%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(x_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names, scores*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement\n",
    "Deep Learning techniques tend to outperform every other algorithm when we get access to lots of data. So if we could train our model on a larger dataset, the model should definitely improve. With RNN, the model creates it's own representation of the sentence. The reviews contain vocabulary specific to the food product. If we had mode data, our model should be able to identify the specific characteristics that make a good product. In order to improve this model, we should also investigate if we could include the product name/product type. If we had this information maybe our RNN would be able to more easily identify the important characteristics for each product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# REFERENCES\n",
    "\n",
    "1.) Burke, R., Hybrid Recommender Systems: Survey and Experiments, <http://josquin.cs.depaul.edu/rburke/pubs/burke-umuai02.pdf>.\n",
    "\n",
    "2.) Gunawardana A., Shani G, Evaluating Recommendation Systems, <http://research.microsoft.com/pubs/115396/evaluationmetrics.tr.pdf>.\n",
    "\n",
    "3.) Wikipedia, Singular Value Decomposition, <http://en.wikipedia.org/ wiki/Singular value decomposition>.\n",
    "\n",
    "4.) Koren, Y., Factorization Meets the Neighborhood: a Multifaceted Col-laborative Filtering Model,<http://public.research.att.com/ volinsky/netflix/kdd08koren.pdf>.\n",
    "\n",
    "5.) Shang, M., Fu, Y., Chen, D., Personal Recommendation Using Weighted BiPartite Graph Projection, Apperceiving Computing and Intelligence Analysis, 2008. ICACIA 2008. International Conference on , vol., no., pp.198,202,\n",
    "13-15 Dec. 2008\n",
    "\n",
    "6.) Breese, J.S., Heckerman, D., Kadie, C., Empirical Analysis of Predictive Algorithms for Collaborative Filtering, <http://research.microsoft.com/ pubs/69656/tr-98-12.pdf>\n",
    "\n",
    "\n",
    "# LICENSE : \n",
    "\n",
    "The code in the document by Ronak Shingala and Spurthi Shetty is licensed under the MIT License https://opensource.org/licenses/MIT\n",
    "\n",
    "https://github.com/shingalaronak/Advance-Data-Science-Final-Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
